{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "reinforcement\n",
        " learning program that reduces entropy in a closed system\n"
      ],
      "metadata": {
        "id": "B2TXDj7oBrAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "# Test the environment\n",
        "env = GridEnvironment()\n",
        "state = env.reset()\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpTkTTySBr5l",
        "outputId": "2518d654-0470-471d-a46b-7ce65362b483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1]\n",
            " [0 1 0 1]\n",
            " [0 1 0 1]\n",
            " [0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "# Initialize agent\n",
        "agent = QLearningAgent(state_shape=(16,), action_size=16)\n"
      ],
      "metadata": {
        "id": "_LNL1GRfBuTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 10\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn3sdl4lBxpF",
        "outputId": "a47f1293-0b8a-48e6-acd0-9eb7a6da0dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 - Steps: 47 - Epsilon: 1.00\n",
            "Episode 2/10 - Steps: 35 - Epsilon: 0.99\n",
            "Episode 3/10 - Steps: 30 - Epsilon: 0.99\n",
            "Episode 4/10 - Steps: 34 - Epsilon: 0.99\n",
            "Episode 5/10 - Steps: 33 - Epsilon: 0.98\n",
            "Episode 6/10 - Steps: 12 - Epsilon: 0.98\n",
            "Episode 7/10 - Steps: 25 - Epsilon: 0.97\n",
            "Episode 8/10 - Steps: 34 - Epsilon: 0.97\n",
            "Episode 9/10 - Steps: 42 - Epsilon: 0.96\n",
            "Episode 10/10 - Steps: 46 - Epsilon: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(agent, env, episodes=10):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            # Predict the best action using the trained model\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Test the trained agent\n",
        "test_agent(agent, env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "p2js08opBzau",
        "outputId": "66dda044-fb82-4ed7-b5d4-a7f13be41405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4130f4b4e2a5>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Test the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4130f4b4e2a5>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, episodes)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Predict the best action using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    743\u001b[0m             self._flat_output_types)\n\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3422\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3423\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "def test_agent(agent, env, episodes=10):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = GridEnvironment()\n",
        "agent = QLearningAgent(state_shape=(16,), action_size=16)\n",
        "\n",
        "# Training the agent\n",
        "episodes = 1000\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Test the trained agent\n",
        "test_agent(agent, env)\n"
      ],
      "metadata": {
        "id": "v--avSIeIla_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=2):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "def test_agent(agent, env, episodes=1):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "# Training the agent\n",
        "episodes = 10\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Test the trained agent\n",
        "test_agent(agent, env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "xc7z1BpJI5EX",
        "outputId": "023ec793-d013-4e01-ae31-28a289214371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 - Steps: 10 - Epsilon: 1.00\n",
            "Episode 2/10 - Steps: 7 - Epsilon: 0.99\n",
            "Episode 3/10 - Steps: 8 - Epsilon: 0.99\n",
            "Episode 4/10 - Steps: 3 - Epsilon: 0.99\n",
            "Episode 5/10 - Steps: 8 - Epsilon: 0.98\n",
            "Episode 6/10 - Steps: 3 - Epsilon: 0.98\n",
            "Episode 7/10 - Steps: 7 - Epsilon: 0.97\n",
            "Episode 8/10 - Steps: 3 - Epsilon: 0.97\n",
            "Episode 9/10 - Steps: 3 - Epsilon: 0.96\n",
            "Episode 10/10 - Steps: 13 - Epsilon: 0.96\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-b16779180adc>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;31m# Test the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-b16779180adc>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, episodes)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2618\u001b[0m                     )\n\u001b[1;32m   2619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   2621\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorExactEvalDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1293\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2323\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mflat_map_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2326\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_flat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;34m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_FlatMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/flat_map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m     34\u001b[0m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1227\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1228\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m           \u001b[0mtarget_func_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_func_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         concrete_function = _create_concrete_function(\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0mtarget_func_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_func_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    308\u001b[0m       \u001b[0mattributes_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLE_ACD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   )\n\u001b[0;32m--> 310\u001b[0;31m   traced_func_graph = func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m    311\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0mtracing_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TensorArrays and `None`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m     func_outputs = nest.map_structure(\n\u001b[0m\u001b[1;32m   1065\u001b[0m         convert, func_outputs, expand_composites=True)\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m   \"\"\"\n\u001b[0;32m--> 631\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    632\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1064\u001b[0m   \"\"\"\n\u001b[1;32m   1065\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1104\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1053\u001b[0m               \"ExtensionType.\")\n\u001b[1;32m   1054\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeps_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36mmark_as_return\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;31m# of a new identity operation that the stateful operations definitely don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;31m# depend on.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_returned_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   4195\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4196\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4197\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   4198\u001b[0m         \"Identity\", input=input, name=name)\n\u001b[1;32m   4199\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m   if (_CanExtractAttrsFastPath(op_def, keywords) and\n\u001b[0;32m--> 764\u001b[0;31m       flags.config().graph_building_optimization.value()):\n\u001b[0m\u001b[1;32m    765\u001b[0m     \u001b[0mfallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     attr_protos, inputs, input_types, output_structure = (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=2):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "def test_agent(agent, env, episodes=1):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            try:\n",
        "                action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "                next_state, reward, done = env.step(action)\n",
        "                state = next_state\n",
        "                steps += 1\n",
        "\n",
        "                if done:\n",
        "                    steps_list.append(steps)\n",
        "                    if np.all(state == 0):\n",
        "                        success_count += 1\n",
        "                    print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                    env.render()\n",
        "                    break\n",
        "            except Exception as ex:\n",
        "                print(f\"An error occurred: {ex}\")\n",
        "                break\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "# Training the agent\n",
        "episodes = 10\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Test the trained agent\n",
        "test_agent(agent, env)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZFCvlVuKOJf",
        "outputId": "464f54c2-4fb9-480c-f110-637a4913854c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 - Steps: 2 - Epsilon: 1.00\n",
            "Episode 2/10 - Steps: 10 - Epsilon: 0.99\n",
            "Episode 3/10 - Steps: 2 - Epsilon: 0.99\n",
            "Episode 4/10 - Steps: 13 - Epsilon: 0.99\n",
            "Episode 5/10 - Steps: 5 - Epsilon: 0.98\n",
            "Episode 6/10 - Steps: 1 - Epsilon: 0.98\n",
            "Episode 7/10 - Steps: 8 - Epsilon: 0.97\n",
            "Episode 8/10 - Steps: 5 - Epsilon: 0.97\n",
            "Episode 9/10 - Steps: 3 - Epsilon: 0.96\n",
            "Episode 10/10 - Steps: 5 - Epsilon: 0.96\n",
            "Test Episode 1/1 - Steps: 1\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Success rate: 100.00%\n",
            "Average steps per successful episode: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=2):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "def test_agent(agent, env, episodes=10):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done:\n",
        "            print(f\"Test Episode {e+1}/{episodes} - Step {steps}\")\n",
        "            print(f\"Current State:\\n{state.reshape(env.size, env.size)}\")\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            print(f\"Chosen Action: {action}\")\n",
        "            next_state, reward, done = env.step(action)\n",
        "            print(f\"Next State:\\n{next_state.reshape(env.size, env.size)}\")\n",
        "            print(f\"Reward: {reward}, Done: {done}\")\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "# Training the agent\n",
        "episodes = 10\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Test the trained agent\n",
        "test_agent(agent, env, episodes=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sj50OOphMpdg",
        "outputId": "de2ac0e9-f5b9-41e9-a97f-17daa6e5d190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 - Steps: 3 - Epsilon: 1.00\n",
            "Episode 2/10 - Steps: 3 - Epsilon: 0.99\n",
            "Episode 3/10 - Steps: 15 - Epsilon: 0.99\n",
            "Episode 4/10 - Steps: 7 - Epsilon: 0.99\n",
            "Episode 5/10 - Steps: 17 - Epsilon: 0.98\n",
            "Episode 6/10 - Steps: 3 - Epsilon: 0.98\n",
            "Episode 7/10 - Steps: 1 - Epsilon: 0.97\n",
            "Episode 8/10 - Steps: 6 - Epsilon: 0.97\n",
            "Episode 9/10 - Steps: 6 - Epsilon: 0.96\n",
            "Episode 10/10 - Steps: 6 - Epsilon: 0.96\n",
            "Test Episode 1/2 - Step 0\n",
            "Current State:\n",
            "[[1 0]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Reward: 1, Done: True\n",
            "Test Episode 1/2 - Steps: 1\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 2/2 - Step 0\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 1\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 2\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 3\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 4\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 5\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 6\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 7\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 8\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 9\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 10\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 11\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 12\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 13\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 14\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 15\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 16\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 17\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 18\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 19\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 20\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 21\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 22\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 23\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 24\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 25\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 26\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 27\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 28\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 29\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 30\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 31\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 32\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 33\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 34\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 35\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 36\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 37\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 38\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 39\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 40\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 41\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 42\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 43\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 44\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 45\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 46\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 47\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 48\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 49\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 50\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 51\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 52\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 53\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 54\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 55\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 56\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 57\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 58\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 59\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 60\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 61\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 62\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 63\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 64\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 65\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 66\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 67\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 68\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 69\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 70\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 71\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 72\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 73\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 74\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 75\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 76\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 77\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 78\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 79\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 80\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 81\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 82\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 83\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 84\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 85\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 86\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 87\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 88\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 89\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 90\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 91\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 92\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 93\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 94\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 95\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 96\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 97\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 98\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 99\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 100\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 101\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 102\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 103\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 104\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 105\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 106\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 107\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 108\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 109\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 110\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 111\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 112\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 113\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 114\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 115\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 116\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 117\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 118\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 119\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 120\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 121\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 122\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 123\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 124\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 125\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 126\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 127\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 128\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 129\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 130\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 131\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 132\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 133\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 134\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 135\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 136\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 137\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 138\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 139\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 140\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 141\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 142\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 143\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 144\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 145\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 146\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 147\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 148\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 149\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 150\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 151\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 152\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 153\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 154\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 155\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 156\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 157\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 158\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 159\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 160\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 161\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 162\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 163\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 164\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 165\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 166\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 167\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 168\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 169\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 170\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 171\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 172\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 173\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 174\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 175\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 176\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 177\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 178\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 179\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 180\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 181\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 182\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 183\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 184\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 185\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 186\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 187\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 188\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 189\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 190\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 191\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 192\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 193\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 194\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 195\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 196\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 197\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 198\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 199\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 200\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 201\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 202\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 203\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 204\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 205\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 206\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 207\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 208\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 209\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 210\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 211\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 212\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 213\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 214\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 215\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 216\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 217\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 218\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 219\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 220\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 221\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 222\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 223\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 224\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 225\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 226\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 227\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 228\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 229\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 230\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 231\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 232\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 233\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 234\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 235\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 236\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 237\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 238\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 239\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 240\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 241\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 242\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 243\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 244\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 245\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 246\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 247\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 248\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 249\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 250\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 251\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 252\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 253\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 254\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 255\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 256\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 257\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 258\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 259\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 260\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 261\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 262\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 263\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 264\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 265\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 266\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 267\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 268\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 269\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 270\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 271\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 272\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 273\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 274\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 275\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 0]\n",
            " [1 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 276\n",
            "Current State:\n",
            "[[0 0]\n",
            " [1 0]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-83a8e92a956b>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;31m# Test the trained agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mtest_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-83a8e92a956b>\u001b[0m in \u001b[0;36mtest_agent\u001b[0;34m(agent, env, episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Episode {e+1}/{episodes} - Step {steps}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Current State:\\n{state.reshape(env.size, env.size)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Chosen Action: {action}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    743\u001b[0m             self._flat_output_types)\n\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3422\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3423\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=2):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "        reward = 1 if self.state[row, col] == 0 else -1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n",
        "\n",
        "def test_agent(agent, env, episodes=10, max_steps_per_episode=100):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps_per_episode:\n",
        "            print(f\"Test Episode {e+1}/{episodes} - Step {steps}\")\n",
        "            print(f\"Current State:\\n{state.reshape(env.size, env.size)}\")\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            print(f\"Chosen Action: {action}\")\n",
        "            next_state, reward, done = env.step(action)\n",
        "            print(f\"Next State:\\n{next_state.reshape(env.size, env.size)}\")\n",
        "            print(f\"Reward: {reward}, Done: {done}\")\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Test Episode {e+1}/{episodes} reached max steps limit\")\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "# Initialize environment and agent\n",
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "# Training the agent\n",
        "episodes = 100\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Test the trained agent with max steps limit\n",
        "test_agent(agent, env, episodes=2, max_steps_per_episode=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aitt-s-GNcav",
        "outputId": "3888d54f-42cd-4546-89c5-0c40f5cc761d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 - Steps: 4 - Epsilon: 1.00\n",
            "Episode 2/100 - Steps: 3 - Epsilon: 0.99\n",
            "Episode 3/100 - Steps: 2 - Epsilon: 0.99\n",
            "Episode 4/100 - Steps: 14 - Epsilon: 0.99\n",
            "Episode 5/100 - Steps: 7 - Epsilon: 0.98\n",
            "Episode 6/100 - Steps: 5 - Epsilon: 0.98\n",
            "Episode 7/100 - Steps: 6 - Epsilon: 0.97\n",
            "Episode 8/100 - Steps: 3 - Epsilon: 0.97\n",
            "Episode 9/100 - Steps: 18 - Epsilon: 0.96\n",
            "Episode 10/100 - Steps: 6 - Epsilon: 0.96\n",
            "Episode 11/100 - Steps: 7 - Epsilon: 0.95\n",
            "Episode 12/100 - Steps: 2 - Epsilon: 0.95\n",
            "Episode 13/100 - Steps: 20 - Epsilon: 0.94\n",
            "Episode 14/100 - Steps: 11 - Epsilon: 0.94\n",
            "Episode 15/100 - Steps: 9 - Epsilon: 0.93\n",
            "Episode 16/100 - Steps: 1 - Epsilon: 0.93\n",
            "Episode 17/100 - Steps: 5 - Epsilon: 0.92\n",
            "Episode 18/100 - Steps: 3 - Epsilon: 0.92\n",
            "Episode 19/100 - Steps: 3 - Epsilon: 0.91\n",
            "Episode 20/100 - Steps: 2 - Epsilon: 0.91\n",
            "Episode 21/100 - Steps: 5 - Epsilon: 0.90\n",
            "Episode 22/100 - Steps: 12 - Epsilon: 0.90\n",
            "Episode 23/100 - Steps: 4 - Epsilon: 0.90\n",
            "Episode 24/100 - Steps: 1 - Epsilon: 0.89\n",
            "Episode 25/100 - Steps: 4 - Epsilon: 0.89\n",
            "Episode 26/100 - Steps: 3 - Epsilon: 0.88\n",
            "Episode 27/100 - Steps: 13 - Epsilon: 0.88\n",
            "Episode 28/100 - Steps: 8 - Epsilon: 0.87\n",
            "Episode 29/100 - Steps: 4 - Epsilon: 0.87\n",
            "Episode 30/100 - Steps: 7 - Epsilon: 0.86\n",
            "Episode 31/100 - Steps: 7 - Epsilon: 0.86\n",
            "Episode 32/100 - Steps: 2 - Epsilon: 0.86\n",
            "Episode 33/100 - Steps: 4 - Epsilon: 0.85\n",
            "Episode 34/100 - Steps: 11 - Epsilon: 0.85\n",
            "Episode 35/100 - Steps: 3 - Epsilon: 0.84\n",
            "Episode 36/100 - Steps: 10 - Epsilon: 0.84\n",
            "Episode 37/100 - Steps: 12 - Epsilon: 0.83\n",
            "Episode 38/100 - Steps: 1 - Epsilon: 0.83\n",
            "Episode 39/100 - Steps: 7 - Epsilon: 0.83\n",
            "Episode 40/100 - Steps: 7 - Epsilon: 0.82\n",
            "Episode 41/100 - Steps: 2 - Epsilon: 0.82\n",
            "Episode 42/100 - Steps: 6 - Epsilon: 0.81\n",
            "Episode 43/100 - Steps: 10 - Epsilon: 0.81\n",
            "Episode 44/100 - Steps: 5 - Epsilon: 0.81\n",
            "Episode 45/100 - Steps: 4 - Epsilon: 0.80\n",
            "Episode 46/100 - Steps: 7 - Epsilon: 0.80\n",
            "Episode 47/100 - Steps: 5 - Epsilon: 0.79\n",
            "Episode 48/100 - Steps: 8 - Epsilon: 0.79\n",
            "Episode 49/100 - Steps: 8 - Epsilon: 0.79\n",
            "Episode 50/100 - Steps: 5 - Epsilon: 0.78\n",
            "Episode 51/100 - Steps: 4 - Epsilon: 0.78\n",
            "Episode 52/100 - Steps: 4 - Epsilon: 0.77\n",
            "Episode 53/100 - Steps: 2 - Epsilon: 0.77\n",
            "Episode 54/100 - Steps: 16 - Epsilon: 0.77\n",
            "Episode 55/100 - Steps: 9 - Epsilon: 0.76\n",
            "Episode 56/100 - Steps: 3 - Epsilon: 0.76\n",
            "Episode 57/100 - Steps: 16 - Epsilon: 0.76\n",
            "Episode 58/100 - Steps: 3 - Epsilon: 0.75\n",
            "Episode 59/100 - Steps: 1 - Epsilon: 0.75\n",
            "Episode 60/100 - Steps: 15 - Epsilon: 0.74\n",
            "Episode 61/100 - Steps: 2 - Epsilon: 0.74\n",
            "Episode 62/100 - Steps: 11 - Epsilon: 0.74\n",
            "Episode 63/100 - Steps: 4 - Epsilon: 0.73\n",
            "Episode 64/100 - Steps: 6 - Epsilon: 0.73\n",
            "Episode 65/100 - Steps: 29 - Epsilon: 0.73\n",
            "Episode 66/100 - Steps: 4 - Epsilon: 0.72\n",
            "Episode 67/100 - Steps: 8 - Epsilon: 0.72\n",
            "Episode 68/100 - Steps: 6 - Epsilon: 0.71\n",
            "Episode 69/100 - Steps: 8 - Epsilon: 0.71\n",
            "Episode 70/100 - Steps: 3 - Epsilon: 0.71\n",
            "Episode 71/100 - Steps: 1 - Epsilon: 0.70\n",
            "Episode 72/100 - Steps: 8 - Epsilon: 0.70\n",
            "Episode 73/100 - Steps: 5 - Epsilon: 0.70\n",
            "Episode 74/100 - Steps: 4 - Epsilon: 0.69\n",
            "Episode 75/100 - Steps: 13 - Epsilon: 0.69\n",
            "Episode 76/100 - Steps: 3 - Epsilon: 0.69\n",
            "Episode 77/100 - Steps: 6 - Epsilon: 0.68\n",
            "Episode 78/100 - Steps: 34 - Epsilon: 0.68\n",
            "Episode 79/100 - Steps: 5 - Epsilon: 0.68\n",
            "Episode 80/100 - Steps: 2 - Epsilon: 0.67\n",
            "Episode 81/100 - Steps: 1 - Epsilon: 0.67\n",
            "Episode 82/100 - Steps: 4 - Epsilon: 0.67\n",
            "Episode 83/100 - Steps: 4 - Epsilon: 0.66\n",
            "Episode 84/100 - Steps: 3 - Epsilon: 0.66\n",
            "Episode 85/100 - Steps: 4 - Epsilon: 0.66\n",
            "Episode 86/100 - Steps: 8 - Epsilon: 0.65\n",
            "Episode 87/100 - Steps: 8 - Epsilon: 0.65\n",
            "Episode 88/100 - Steps: 6 - Epsilon: 0.65\n",
            "Episode 89/100 - Steps: 12 - Epsilon: 0.64\n",
            "Episode 90/100 - Steps: 29 - Epsilon: 0.64\n",
            "Episode 91/100 - Steps: 12 - Epsilon: 0.64\n",
            "Episode 92/100 - Steps: 16 - Epsilon: 0.63\n",
            "Episode 93/100 - Steps: 2 - Epsilon: 0.63\n",
            "Episode 94/100 - Steps: 10 - Epsilon: 0.63\n",
            "Episode 95/100 - Steps: 6 - Epsilon: 0.62\n",
            "Episode 96/100 - Steps: 3 - Epsilon: 0.62\n",
            "Episode 97/100 - Steps: 15 - Epsilon: 0.62\n",
            "Episode 98/100 - Steps: 1 - Epsilon: 0.61\n",
            "Episode 99/100 - Steps: 5 - Epsilon: 0.61\n",
            "Episode 100/100 - Steps: 15 - Epsilon: 0.61\n",
            "Test Episode 1/2 - Step 0\n",
            "Current State:\n",
            "[[1 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 1\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 2\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 3\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 4\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 5\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 6\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 7\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 8\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 9\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 10\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 11\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 12\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 13\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 14\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 15\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 16\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 17\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 18\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 19\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 20\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 21\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 22\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 23\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 24\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 25\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 26\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 27\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 28\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 29\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 30\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 31\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 32\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 33\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 34\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 35\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 36\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 37\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 38\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 39\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 40\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 41\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 42\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 43\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 44\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 45\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 46\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 47\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 48\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 49\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 50\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 51\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 52\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 53\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 54\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 55\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 56\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 57\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 58\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 59\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 60\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 61\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 62\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 63\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 64\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 65\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 66\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 67\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 68\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 69\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 70\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 71\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 72\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 73\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 74\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 75\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 76\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 77\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 78\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 79\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 80\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 81\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 82\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 83\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 84\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 85\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 86\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 87\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 88\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 89\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 90\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 91\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 92\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 93\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 94\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 95\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 96\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 97\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 98\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 - Step 99\n",
            "Current State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [1 1]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 1/2 reached max steps limit\n",
            "Test Episode 2/2 - Step 0\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 1\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 2\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 3\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 4\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 5\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 6\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 7\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 8\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 9\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 10\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 11\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 12\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 13\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 14\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 15\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 16\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 17\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 18\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 19\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 20\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 21\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 22\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 23\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 24\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 25\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 26\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 27\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 28\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 29\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 30\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 31\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 32\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 33\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 34\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 35\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 36\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 37\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 38\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 39\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 40\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 41\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 42\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 43\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 44\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 45\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 46\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 47\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 48\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 49\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 50\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 51\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 52\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 53\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 54\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 55\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 56\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 57\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 58\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 59\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 60\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 61\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 62\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 63\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 64\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 65\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 66\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 67\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 68\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 69\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 70\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 71\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 72\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 73\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 74\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 75\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 76\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 77\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 78\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 79\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 80\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 81\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 82\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 83\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 84\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 85\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 86\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 87\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 88\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 89\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 90\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 91\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 92\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 93\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 94\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 95\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 96\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 97\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 98\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 - Step 99\n",
            "Current State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Chosen Action: 0\n",
            "Next State:\n",
            "[[0 1]\n",
            " [0 0]]\n",
            "Reward: 1, Done: False\n",
            "Test Episode 2/2 reached max steps limit\n",
            "Success rate: 0.00%\n",
            "Average steps per successful episode: N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, size=2):\n",
        "        self.size = size\n",
        "        self.state = np.random.randint(2, size=(size, size))\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=(self.size, self.size))\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        row, col = divmod(action, self.size)\n",
        "        reward = 0\n",
        "        if self.state[row, col] == 1:\n",
        "            self.state[row, col] = 0\n",
        "            reward = 1\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n"
      ],
      "metadata": {
        "id": "9Al4iEYYPnYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n"
      ],
      "metadata": {
        "id": "oPoxNuviPoqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "episodes = 100\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc4M1ce9PrKI",
        "outputId": "08f87a35-9fae-4f35-9459-683f7e2d5a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/100 - Steps: 8 - Epsilon: 1.00\n",
            "Episode 2/100 - Steps: 5 - Epsilon: 0.99\n",
            "Episode 3/100 - Steps: 3 - Epsilon: 0.99\n",
            "Episode 4/100 - Steps: 16 - Epsilon: 0.99\n",
            "Episode 5/100 - Steps: 15 - Epsilon: 0.98\n",
            "Episode 6/100 - Steps: 7 - Epsilon: 0.98\n",
            "Episode 7/100 - Steps: 6 - Epsilon: 0.97\n",
            "Episode 8/100 - Steps: 3 - Epsilon: 0.97\n",
            "Episode 9/100 - Steps: 6 - Epsilon: 0.96\n",
            "Episode 10/100 - Steps: 8 - Epsilon: 0.96\n",
            "Episode 11/100 - Steps: 12 - Epsilon: 0.95\n",
            "Episode 12/100 - Steps: 6 - Epsilon: 0.95\n",
            "Episode 13/100 - Steps: 7 - Epsilon: 0.94\n",
            "Episode 14/100 - Steps: 2 - Epsilon: 0.94\n",
            "Episode 15/100 - Steps: 9 - Epsilon: 0.93\n",
            "Episode 16/100 - Steps: 8 - Epsilon: 0.93\n",
            "Episode 17/100 - Steps: 8 - Epsilon: 0.92\n",
            "Episode 18/100 - Steps: 1 - Epsilon: 0.92\n",
            "Episode 19/100 - Steps: 7 - Epsilon: 0.91\n",
            "Episode 20/100 - Steps: 10 - Epsilon: 0.91\n",
            "Episode 21/100 - Steps: 5 - Epsilon: 0.90\n",
            "Episode 22/100 - Steps: 4 - Epsilon: 0.90\n",
            "Episode 23/100 - Steps: 1 - Epsilon: 0.90\n",
            "Episode 24/100 - Steps: 8 - Epsilon: 0.89\n",
            "Episode 25/100 - Steps: 5 - Epsilon: 0.89\n",
            "Episode 26/100 - Steps: 5 - Epsilon: 0.88\n",
            "Episode 27/100 - Steps: 5 - Epsilon: 0.88\n",
            "Episode 28/100 - Steps: 4 - Epsilon: 0.87\n",
            "Episode 29/100 - Steps: 8 - Epsilon: 0.87\n",
            "Episode 30/100 - Steps: 4 - Epsilon: 0.86\n",
            "Episode 31/100 - Steps: 1 - Epsilon: 0.86\n",
            "Episode 32/100 - Steps: 1 - Epsilon: 0.86\n",
            "Episode 33/100 - Steps: 4 - Epsilon: 0.85\n",
            "Episode 34/100 - Steps: 8 - Epsilon: 0.85\n",
            "Episode 35/100 - Steps: 6 - Epsilon: 0.84\n",
            "Episode 36/100 - Steps: 2 - Epsilon: 0.84\n",
            "Episode 37/100 - Steps: 2 - Epsilon: 0.83\n",
            "Episode 38/100 - Steps: 2 - Epsilon: 0.83\n",
            "Episode 39/100 - Steps: 4 - Epsilon: 0.83\n",
            "Episode 40/100 - Steps: 4 - Epsilon: 0.82\n",
            "Episode 41/100 - Steps: 5 - Epsilon: 0.82\n",
            "Episode 42/100 - Steps: 5 - Epsilon: 0.81\n",
            "Episode 43/100 - Steps: 4 - Epsilon: 0.81\n",
            "Episode 44/100 - Steps: 3 - Epsilon: 0.81\n",
            "Episode 45/100 - Steps: 5 - Epsilon: 0.80\n",
            "Episode 46/100 - Steps: 9 - Epsilon: 0.80\n",
            "Episode 47/100 - Steps: 9 - Epsilon: 0.79\n",
            "Episode 48/100 - Steps: 7 - Epsilon: 0.79\n",
            "Episode 49/100 - Steps: 3 - Epsilon: 0.79\n",
            "Episode 50/100 - Steps: 7 - Epsilon: 0.78\n",
            "Episode 51/100 - Steps: 13 - Epsilon: 0.78\n",
            "Episode 52/100 - Steps: 6 - Epsilon: 0.77\n",
            "Episode 53/100 - Steps: 8 - Epsilon: 0.77\n",
            "Episode 54/100 - Steps: 14 - Epsilon: 0.77\n",
            "Episode 55/100 - Steps: 1 - Epsilon: 0.76\n",
            "Episode 56/100 - Steps: 6 - Epsilon: 0.76\n",
            "Episode 57/100 - Steps: 13 - Epsilon: 0.76\n",
            "Episode 58/100 - Steps: 5 - Epsilon: 0.75\n",
            "Episode 59/100 - Steps: 8 - Epsilon: 0.75\n",
            "Episode 60/100 - Steps: 6 - Epsilon: 0.74\n",
            "Episode 61/100 - Steps: 12 - Epsilon: 0.74\n",
            "Episode 62/100 - Steps: 7 - Epsilon: 0.74\n",
            "Episode 63/100 - Steps: 19 - Epsilon: 0.73\n",
            "Episode 64/100 - Steps: 5 - Epsilon: 0.73\n",
            "Episode 65/100 - Steps: 4 - Epsilon: 0.73\n",
            "Episode 66/100 - Steps: 5 - Epsilon: 0.72\n",
            "Episode 67/100 - Steps: 4 - Epsilon: 0.72\n",
            "Episode 68/100 - Steps: 8 - Epsilon: 0.71\n",
            "Episode 69/100 - Steps: 5 - Epsilon: 0.71\n",
            "Episode 70/100 - Steps: 9 - Epsilon: 0.71\n",
            "Episode 71/100 - Steps: 10 - Epsilon: 0.70\n",
            "Episode 72/100 - Steps: 16 - Epsilon: 0.70\n",
            "Episode 73/100 - Steps: 7 - Epsilon: 0.70\n",
            "Episode 74/100 - Steps: 6 - Epsilon: 0.69\n",
            "Episode 75/100 - Steps: 18 - Epsilon: 0.69\n",
            "Episode 76/100 - Steps: 2 - Epsilon: 0.69\n",
            "Episode 77/100 - Steps: 7 - Epsilon: 0.68\n",
            "Episode 78/100 - Steps: 12 - Epsilon: 0.68\n",
            "Episode 79/100 - Steps: 15 - Epsilon: 0.68\n",
            "Episode 80/100 - Steps: 12 - Epsilon: 0.67\n",
            "Episode 81/100 - Steps: 9 - Epsilon: 0.67\n",
            "Episode 82/100 - Steps: 10 - Epsilon: 0.67\n",
            "Episode 83/100 - Steps: 4 - Epsilon: 0.66\n",
            "Episode 84/100 - Steps: 7 - Epsilon: 0.66\n",
            "Episode 85/100 - Steps: 4 - Epsilon: 0.66\n",
            "Episode 86/100 - Steps: 8 - Epsilon: 0.65\n",
            "Episode 87/100 - Steps: 2 - Epsilon: 0.65\n",
            "Episode 88/100 - Steps: 7 - Epsilon: 0.65\n",
            "Episode 89/100 - Steps: 9 - Epsilon: 0.64\n",
            "Episode 90/100 - Steps: 1 - Epsilon: 0.64\n",
            "Episode 91/100 - Steps: 3 - Epsilon: 0.64\n",
            "Episode 92/100 - Steps: 3 - Epsilon: 0.63\n",
            "Episode 93/100 - Steps: 9 - Epsilon: 0.63\n",
            "Episode 94/100 - Steps: 22 - Epsilon: 0.63\n",
            "Episode 95/100 - Steps: 32 - Epsilon: 0.62\n",
            "Episode 96/100 - Steps: 16 - Epsilon: 0.62\n",
            "Episode 97/100 - Steps: 1 - Epsilon: 0.62\n",
            "Episode 98/100 - Steps: 7 - Epsilon: 0.61\n",
            "Episode 99/100 - Steps: 3 - Epsilon: 0.61\n",
            "Episode 100/100 - Steps: 7 - Epsilon: 0.61\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(agent, env, episodes=10, max_steps_per_episode=100):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps_per_episode:\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Test Episode {e+1}/{episodes} reached max steps limit\")\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "test_agent(agent, env, episodes=2, max_steps_per_episode=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKoPpO5HPsvH",
        "outputId": "7a2db444-af9f-4531-9b04-5034c0eda7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Episode 1/2 reached max steps limit\n",
            "Test Episode 2/2 reached max steps limit\n",
            "Success rate: 0.00%\n",
            "Average steps per successful episode: N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n"
      ],
      "metadata": {
        "id": "pXMhBfE5QJ6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridEnvironment(size=2)\n",
        "agent = QLearningAgent(state_shape=(4,), action_size=4)\n",
        "\n",
        "episodes = 1000\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.99\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvDa9e0QKpX",
        "outputId": "8615e16c-c898-481d-9e62-48d481b5063c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000 - Steps: 10 - Epsilon: 1.00\n",
            "Episode 2/1000 - Steps: 4 - Epsilon: 0.99\n",
            "Episode 3/1000 - Steps: 14 - Epsilon: 0.98\n",
            "Episode 4/1000 - Steps: 11 - Epsilon: 0.97\n",
            "Episode 5/1000 - Steps: 7 - Epsilon: 0.96\n",
            "Episode 6/1000 - Steps: 4 - Epsilon: 0.95\n",
            "Episode 7/1000 - Steps: 10 - Epsilon: 0.94\n",
            "Episode 8/1000 - Steps: 11 - Epsilon: 0.93\n",
            "Episode 9/1000 - Steps: 6 - Epsilon: 0.92\n",
            "Episode 10/1000 - Steps: 3 - Epsilon: 0.91\n",
            "Episode 11/1000 - Steps: 4 - Epsilon: 0.90\n",
            "Episode 12/1000 - Steps: 5 - Epsilon: 0.90\n",
            "Episode 13/1000 - Steps: 16 - Epsilon: 0.89\n",
            "Episode 14/1000 - Steps: 2 - Epsilon: 0.88\n",
            "Episode 15/1000 - Steps: 5 - Epsilon: 0.87\n",
            "Episode 16/1000 - Steps: 8 - Epsilon: 0.86\n",
            "Episode 17/1000 - Steps: 8 - Epsilon: 0.85\n",
            "Episode 18/1000 - Steps: 5 - Epsilon: 0.84\n",
            "Episode 19/1000 - Steps: 4 - Epsilon: 0.83\n",
            "Episode 20/1000 - Steps: 11 - Epsilon: 0.83\n",
            "Episode 21/1000 - Steps: 5 - Epsilon: 0.82\n",
            "Episode 22/1000 - Steps: 8 - Epsilon: 0.81\n",
            "Episode 23/1000 - Steps: 7 - Epsilon: 0.80\n",
            "Episode 24/1000 - Steps: 3 - Epsilon: 0.79\n",
            "Episode 25/1000 - Steps: 5 - Epsilon: 0.79\n",
            "Episode 26/1000 - Steps: 11 - Epsilon: 0.78\n",
            "Episode 27/1000 - Steps: 3 - Epsilon: 0.77\n",
            "Episode 28/1000 - Steps: 4 - Epsilon: 0.76\n",
            "Episode 29/1000 - Steps: 4 - Epsilon: 0.75\n",
            "Episode 30/1000 - Steps: 3 - Epsilon: 0.75\n",
            "Episode 31/1000 - Steps: 3 - Epsilon: 0.74\n",
            "Episode 32/1000 - Steps: 8 - Epsilon: 0.73\n",
            "Episode 33/1000 - Steps: 4 - Epsilon: 0.72\n",
            "Episode 34/1000 - Steps: 1 - Epsilon: 0.72\n",
            "Episode 35/1000 - Steps: 7 - Epsilon: 0.71\n",
            "Episode 36/1000 - Steps: 1 - Epsilon: 0.70\n",
            "Episode 37/1000 - Steps: 3 - Epsilon: 0.70\n",
            "Episode 38/1000 - Steps: 5 - Epsilon: 0.69\n",
            "Episode 39/1000 - Steps: 7 - Epsilon: 0.68\n",
            "Episode 40/1000 - Steps: 7 - Epsilon: 0.68\n",
            "Episode 41/1000 - Steps: 6 - Epsilon: 0.67\n",
            "Episode 42/1000 - Steps: 9 - Epsilon: 0.66\n",
            "Episode 43/1000 - Steps: 8 - Epsilon: 0.66\n",
            "Episode 44/1000 - Steps: 3 - Epsilon: 0.65\n",
            "Episode 45/1000 - Steps: 4 - Epsilon: 0.64\n",
            "Episode 46/1000 - Steps: 9 - Epsilon: 0.64\n",
            "Episode 47/1000 - Steps: 2 - Epsilon: 0.63\n",
            "Episode 48/1000 - Steps: 4 - Epsilon: 0.62\n",
            "Episode 49/1000 - Steps: 12 - Epsilon: 0.62\n",
            "Episode 50/1000 - Steps: 24 - Epsilon: 0.61\n",
            "Episode 51/1000 - Steps: 15 - Epsilon: 0.61\n",
            "Episode 52/1000 - Steps: 10 - Epsilon: 0.60\n",
            "Episode 53/1000 - Steps: 6 - Epsilon: 0.59\n",
            "Episode 54/1000 - Steps: 6 - Epsilon: 0.59\n",
            "Episode 55/1000 - Steps: 10 - Epsilon: 0.58\n",
            "Episode 56/1000 - Steps: 3 - Epsilon: 0.58\n",
            "Episode 57/1000 - Steps: 4 - Epsilon: 0.57\n",
            "Episode 58/1000 - Steps: 6 - Epsilon: 0.56\n",
            "Episode 59/1000 - Steps: 1 - Epsilon: 0.56\n",
            "Episode 60/1000 - Steps: 5 - Epsilon: 0.55\n",
            "Episode 61/1000 - Steps: 21 - Epsilon: 0.55\n",
            "Episode 62/1000 - Steps: 18 - Epsilon: 0.54\n",
            "Episode 63/1000 - Steps: 1 - Epsilon: 0.54\n",
            "Episode 64/1000 - Steps: 10 - Epsilon: 0.53\n",
            "Episode 65/1000 - Steps: 1 - Epsilon: 0.53\n",
            "Episode 66/1000 - Steps: 9 - Epsilon: 0.52\n",
            "Episode 67/1000 - Steps: 18 - Epsilon: 0.52\n",
            "Episode 68/1000 - Steps: 12 - Epsilon: 0.51\n",
            "Episode 69/1000 - Steps: 14 - Epsilon: 0.50\n",
            "Episode 70/1000 - Steps: 2 - Epsilon: 0.50\n",
            "Episode 71/1000 - Steps: 12 - Epsilon: 0.49\n",
            "Episode 72/1000 - Steps: 6 - Epsilon: 0.49\n",
            "Episode 73/1000 - Steps: 39 - Epsilon: 0.48\n",
            "Episode 74/1000 - Steps: 12 - Epsilon: 0.48\n",
            "Episode 75/1000 - Steps: 8 - Epsilon: 0.48\n",
            "Episode 76/1000 - Steps: 11 - Epsilon: 0.47\n",
            "Episode 77/1000 - Steps: 14 - Epsilon: 0.47\n",
            "Episode 78/1000 - Steps: 13 - Epsilon: 0.46\n",
            "Episode 79/1000 - Steps: 15 - Epsilon: 0.46\n",
            "Episode 80/1000 - Steps: 4 - Epsilon: 0.45\n",
            "Episode 81/1000 - Steps: 11 - Epsilon: 0.45\n",
            "Episode 82/1000 - Steps: 11 - Epsilon: 0.44\n",
            "Episode 83/1000 - Steps: 14 - Epsilon: 0.44\n",
            "Episode 84/1000 - Steps: 8 - Epsilon: 0.43\n",
            "Episode 85/1000 - Steps: 15 - Epsilon: 0.43\n",
            "Episode 86/1000 - Steps: 19 - Epsilon: 0.43\n",
            "Episode 87/1000 - Steps: 1 - Epsilon: 0.42\n",
            "Episode 88/1000 - Steps: 6 - Epsilon: 0.42\n",
            "Episode 89/1000 - Steps: 7 - Epsilon: 0.41\n",
            "Episode 90/1000 - Steps: 13 - Epsilon: 0.41\n",
            "Episode 91/1000 - Steps: 2 - Epsilon: 0.40\n",
            "Episode 92/1000 - Steps: 54 - Epsilon: 0.40\n",
            "Episode 93/1000 - Steps: 5 - Epsilon: 0.40\n",
            "Episode 94/1000 - Steps: 7 - Epsilon: 0.39\n",
            "Episode 95/1000 - Steps: 11 - Epsilon: 0.39\n",
            "Episode 96/1000 - Steps: 24 - Epsilon: 0.38\n",
            "Episode 97/1000 - Steps: 8 - Epsilon: 0.38\n",
            "Episode 98/1000 - Steps: 25 - Epsilon: 0.38\n",
            "Episode 99/1000 - Steps: 11 - Epsilon: 0.37\n",
            "Episode 100/1000 - Steps: 64 - Epsilon: 0.37\n",
            "Episode 101/1000 - Steps: 5 - Epsilon: 0.37\n",
            "Episode 102/1000 - Steps: 7 - Epsilon: 0.36\n",
            "Episode 103/1000 - Steps: 9 - Epsilon: 0.36\n",
            "Episode 104/1000 - Steps: 3 - Epsilon: 0.36\n",
            "Episode 105/1000 - Steps: 4 - Epsilon: 0.35\n",
            "Episode 106/1000 - Steps: 8 - Epsilon: 0.35\n",
            "Episode 107/1000 - Steps: 1 - Epsilon: 0.34\n",
            "Episode 108/1000 - Steps: 35 - Epsilon: 0.34\n",
            "Episode 109/1000 - Steps: 16 - Epsilon: 0.34\n",
            "Episode 110/1000 - Steps: 37 - Epsilon: 0.33\n",
            "Episode 111/1000 - Steps: 9 - Epsilon: 0.33\n",
            "Episode 112/1000 - Steps: 26 - Epsilon: 0.33\n",
            "Episode 113/1000 - Steps: 5 - Epsilon: 0.32\n",
            "Episode 114/1000 - Steps: 20 - Epsilon: 0.32\n",
            "Episode 115/1000 - Steps: 8 - Epsilon: 0.32\n",
            "Episode 116/1000 - Steps: 55 - Epsilon: 0.31\n",
            "Episode 117/1000 - Steps: 64 - Epsilon: 0.31\n",
            "Episode 118/1000 - Steps: 26 - Epsilon: 0.31\n",
            "Episode 119/1000 - Steps: 31 - Epsilon: 0.31\n",
            "Episode 120/1000 - Steps: 11 - Epsilon: 0.30\n",
            "Episode 121/1000 - Steps: 55 - Epsilon: 0.30\n",
            "Episode 122/1000 - Steps: 10 - Epsilon: 0.30\n",
            "Episode 123/1000 - Steps: 31 - Epsilon: 0.29\n",
            "Episode 124/1000 - Steps: 10 - Epsilon: 0.29\n",
            "Episode 125/1000 - Steps: 5 - Epsilon: 0.29\n",
            "Episode 126/1000 - Steps: 13 - Epsilon: 0.28\n",
            "Episode 127/1000 - Steps: 9 - Epsilon: 0.28\n",
            "Episode 128/1000 - Steps: 10 - Epsilon: 0.28\n",
            "Episode 129/1000 - Steps: 23 - Epsilon: 0.28\n",
            "Episode 130/1000 - Steps: 11 - Epsilon: 0.27\n",
            "Episode 131/1000 - Steps: 2 - Epsilon: 0.27\n",
            "Episode 132/1000 - Steps: 2 - Epsilon: 0.27\n",
            "Episode 133/1000 - Steps: 32 - Epsilon: 0.27\n",
            "Episode 134/1000 - Steps: 1 - Epsilon: 0.26\n",
            "Episode 135/1000 - Steps: 33 - Epsilon: 0.26\n",
            "Episode 136/1000 - Steps: 24 - Epsilon: 0.26\n",
            "Episode 137/1000 - Steps: 6 - Epsilon: 0.25\n",
            "Episode 138/1000 - Steps: 12 - Epsilon: 0.25\n",
            "Episode 139/1000 - Steps: 44 - Epsilon: 0.25\n",
            "Episode 140/1000 - Steps: 1 - Epsilon: 0.25\n",
            "Episode 141/1000 - Steps: 34 - Epsilon: 0.24\n",
            "Episode 142/1000 - Steps: 5 - Epsilon: 0.24\n",
            "Episode 143/1000 - Steps: 16 - Epsilon: 0.24\n",
            "Episode 144/1000 - Steps: 69 - Epsilon: 0.24\n",
            "Episode 145/1000 - Steps: 6 - Epsilon: 0.24\n",
            "Episode 146/1000 - Steps: 3 - Epsilon: 0.23\n",
            "Episode 147/1000 - Steps: 2 - Epsilon: 0.23\n",
            "Episode 148/1000 - Steps: 19 - Epsilon: 0.23\n",
            "Episode 149/1000 - Steps: 13 - Epsilon: 0.23\n",
            "Episode 150/1000 - Steps: 7 - Epsilon: 0.22\n",
            "Episode 151/1000 - Steps: 13 - Epsilon: 0.22\n",
            "Episode 152/1000 - Steps: 7 - Epsilon: 0.22\n",
            "Episode 153/1000 - Steps: 41 - Epsilon: 0.22\n",
            "Episode 154/1000 - Steps: 7 - Epsilon: 0.21\n",
            "Episode 155/1000 - Steps: 41 - Epsilon: 0.21\n",
            "Episode 156/1000 - Steps: 46 - Epsilon: 0.21\n",
            "Episode 157/1000 - Steps: 30 - Epsilon: 0.21\n",
            "Episode 158/1000 - Steps: 16 - Epsilon: 0.21\n",
            "Episode 159/1000 - Steps: 5 - Epsilon: 0.20\n",
            "Episode 160/1000 - Steps: 1 - Epsilon: 0.20\n",
            "Episode 161/1000 - Steps: 30 - Epsilon: 0.20\n",
            "Episode 162/1000 - Steps: 61 - Epsilon: 0.20\n",
            "Episode 163/1000 - Steps: 16 - Epsilon: 0.20\n",
            "Episode 164/1000 - Steps: 25 - Epsilon: 0.19\n",
            "Episode 165/1000 - Steps: 10 - Epsilon: 0.19\n",
            "Episode 166/1000 - Steps: 41 - Epsilon: 0.19\n",
            "Episode 167/1000 - Steps: 29 - Epsilon: 0.19\n",
            "Episode 168/1000 - Steps: 83 - Epsilon: 0.19\n",
            "Episode 169/1000 - Steps: 36 - Epsilon: 0.18\n",
            "Episode 170/1000 - Steps: 105 - Epsilon: 0.18\n",
            "Episode 171/1000 - Steps: 13 - Epsilon: 0.18\n",
            "Episode 172/1000 - Steps: 6 - Epsilon: 0.18\n",
            "Episode 173/1000 - Steps: 5 - Epsilon: 0.18\n",
            "Episode 174/1000 - Steps: 14 - Epsilon: 0.18\n",
            "Episode 175/1000 - Steps: 13 - Epsilon: 0.17\n",
            "Episode 176/1000 - Steps: 17 - Epsilon: 0.17\n",
            "Episode 177/1000 - Steps: 14 - Epsilon: 0.17\n",
            "Episode 178/1000 - Steps: 1 - Epsilon: 0.17\n",
            "Episode 179/1000 - Steps: 18 - Epsilon: 0.17\n",
            "Episode 180/1000 - Steps: 1 - Epsilon: 0.17\n",
            "Episode 181/1000 - Steps: 8 - Epsilon: 0.16\n",
            "Episode 182/1000 - Steps: 1 - Epsilon: 0.16\n",
            "Episode 183/1000 - Steps: 3 - Epsilon: 0.16\n",
            "Episode 184/1000 - Steps: 1 - Epsilon: 0.16\n",
            "Episode 185/1000 - Steps: 1 - Epsilon: 0.16\n",
            "Episode 186/1000 - Steps: 23 - Epsilon: 0.16\n",
            "Episode 187/1000 - Steps: 109 - Epsilon: 0.15\n",
            "Episode 188/1000 - Steps: 7 - Epsilon: 0.15\n",
            "Episode 189/1000 - Steps: 72 - Epsilon: 0.15\n",
            "Episode 190/1000 - Steps: 34 - Epsilon: 0.15\n",
            "Episode 191/1000 - Steps: 1 - Epsilon: 0.15\n",
            "Episode 192/1000 - Steps: 45 - Epsilon: 0.15\n",
            "Episode 193/1000 - Steps: 65 - Epsilon: 0.15\n",
            "Episode 194/1000 - Steps: 59 - Epsilon: 0.14\n",
            "Episode 195/1000 - Steps: 102 - Epsilon: 0.14\n",
            "Episode 196/1000 - Steps: 9 - Epsilon: 0.14\n",
            "Episode 197/1000 - Steps: 40 - Epsilon: 0.14\n",
            "Episode 198/1000 - Steps: 11 - Epsilon: 0.14\n",
            "Episode 199/1000 - Steps: 4 - Epsilon: 0.14\n",
            "Episode 200/1000 - Steps: 44 - Epsilon: 0.14\n",
            "Episode 201/1000 - Steps: 1 - Epsilon: 0.13\n",
            "Episode 202/1000 - Steps: 54 - Epsilon: 0.13\n",
            "Episode 203/1000 - Steps: 12 - Epsilon: 0.13\n",
            "Episode 204/1000 - Steps: 22 - Epsilon: 0.13\n",
            "Episode 205/1000 - Steps: 24 - Epsilon: 0.13\n",
            "Episode 206/1000 - Steps: 25 - Epsilon: 0.13\n",
            "Episode 207/1000 - Steps: 35 - Epsilon: 0.13\n",
            "Episode 208/1000 - Steps: 11 - Epsilon: 0.12\n",
            "Episode 209/1000 - Steps: 61 - Epsilon: 0.12\n",
            "Episode 210/1000 - Steps: 1 - Epsilon: 0.12\n",
            "Episode 211/1000 - Steps: 169 - Epsilon: 0.12\n",
            "Episode 212/1000 - Steps: 20 - Epsilon: 0.12\n",
            "Episode 213/1000 - Steps: 17 - Epsilon: 0.12\n",
            "Episode 214/1000 - Steps: 36 - Epsilon: 0.12\n",
            "Episode 215/1000 - Steps: 78 - Epsilon: 0.12\n",
            "Episode 216/1000 - Steps: 43 - Epsilon: 0.12\n",
            "Episode 217/1000 - Steps: 1 - Epsilon: 0.11\n",
            "Episode 218/1000 - Steps: 6 - Epsilon: 0.11\n",
            "Episode 219/1000 - Steps: 1 - Epsilon: 0.11\n",
            "Episode 220/1000 - Steps: 29 - Epsilon: 0.11\n",
            "Episode 221/1000 - Steps: 20 - Epsilon: 0.11\n",
            "Episode 222/1000 - Steps: 4 - Epsilon: 0.11\n",
            "Episode 223/1000 - Steps: 3 - Epsilon: 0.11\n",
            "Episode 224/1000 - Steps: 109 - Epsilon: 0.11\n",
            "Episode 225/1000 - Steps: 15 - Epsilon: 0.11\n",
            "Episode 226/1000 - Steps: 36 - Epsilon: 0.10\n",
            "Episode 227/1000 - Steps: 36 - Epsilon: 0.10\n",
            "Episode 228/1000 - Steps: 42 - Epsilon: 0.10\n",
            "Episode 229/1000 - Steps: 1 - Epsilon: 0.10\n",
            "Episode 230/1000 - Steps: 3 - Epsilon: 0.10\n",
            "Episode 231/1000 - Steps: 12 - Epsilon: 0.10\n",
            "Episode 232/1000 - Steps: 3 - Epsilon: 0.10\n",
            "Episode 233/1000 - Steps: 11 - Epsilon: 0.10\n",
            "Episode 234/1000 - Steps: 3 - Epsilon: 0.10\n",
            "Episode 235/1000 - Steps: 5 - Epsilon: 0.10\n",
            "Episode 236/1000 - Steps: 46 - Epsilon: 0.09\n",
            "Episode 237/1000 - Steps: 29 - Epsilon: 0.09\n",
            "Episode 238/1000 - Steps: 31 - Epsilon: 0.09\n",
            "Episode 239/1000 - Steps: 3 - Epsilon: 0.09\n",
            "Episode 240/1000 - Steps: 3 - Epsilon: 0.09\n",
            "Episode 241/1000 - Steps: 10 - Epsilon: 0.09\n",
            "Episode 242/1000 - Steps: 15 - Epsilon: 0.09\n",
            "Episode 243/1000 - Steps: 2 - Epsilon: 0.09\n",
            "Episode 244/1000 - Steps: 1 - Epsilon: 0.09\n",
            "Episode 245/1000 - Steps: 1 - Epsilon: 0.09\n",
            "Episode 246/1000 - Steps: 24 - Epsilon: 0.09\n",
            "Episode 247/1000 - Steps: 5 - Epsilon: 0.08\n",
            "Episode 248/1000 - Steps: 1 - Epsilon: 0.08\n",
            "Episode 249/1000 - Steps: 2 - Epsilon: 0.08\n",
            "Episode 250/1000 - Steps: 10 - Epsilon: 0.08\n",
            "Episode 251/1000 - Steps: 5 - Epsilon: 0.08\n",
            "Episode 252/1000 - Steps: 1 - Epsilon: 0.08\n",
            "Episode 253/1000 - Steps: 1 - Epsilon: 0.08\n",
            "Episode 254/1000 - Steps: 42 - Epsilon: 0.08\n",
            "Episode 255/1000 - Steps: 2 - Epsilon: 0.08\n",
            "Episode 256/1000 - Steps: 15 - Epsilon: 0.08\n",
            "Episode 257/1000 - Steps: 3 - Epsilon: 0.08\n",
            "Episode 258/1000 - Steps: 3 - Epsilon: 0.08\n",
            "Episode 259/1000 - Steps: 5 - Epsilon: 0.07\n",
            "Episode 260/1000 - Steps: 19 - Epsilon: 0.07\n",
            "Episode 261/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 262/1000 - Steps: 11 - Epsilon: 0.07\n",
            "Episode 263/1000 - Steps: 26 - Epsilon: 0.07\n",
            "Episode 264/1000 - Steps: 2 - Epsilon: 0.07\n",
            "Episode 265/1000 - Steps: 2 - Epsilon: 0.07\n",
            "Episode 266/1000 - Steps: 2 - Epsilon: 0.07\n",
            "Episode 267/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 268/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 269/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 270/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 271/1000 - Steps: 1 - Epsilon: 0.07\n",
            "Episode 272/1000 - Steps: 2 - Epsilon: 0.07\n",
            "Episode 273/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 274/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 275/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 276/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 277/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 278/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 279/1000 - Steps: 3 - Epsilon: 0.06\n",
            "Episode 280/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 281/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 282/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 283/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 284/1000 - Steps: 3 - Epsilon: 0.06\n",
            "Episode 285/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 286/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 287/1000 - Steps: 1 - Epsilon: 0.06\n",
            "Episode 288/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 289/1000 - Steps: 2 - Epsilon: 0.06\n",
            "Episode 290/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 291/1000 - Steps: 1 - Epsilon: 0.05\n",
            "Episode 292/1000 - Steps: 4 - Epsilon: 0.05\n",
            "Episode 293/1000 - Steps: 3 - Epsilon: 0.05\n",
            "Episode 294/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 295/1000 - Steps: 30 - Epsilon: 0.05\n",
            "Episode 296/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 297/1000 - Steps: 3 - Epsilon: 0.05\n",
            "Episode 298/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 299/1000 - Steps: 3 - Epsilon: 0.05\n",
            "Episode 300/1000 - Steps: 1 - Epsilon: 0.05\n",
            "Episode 301/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 302/1000 - Steps: 1 - Epsilon: 0.05\n",
            "Episode 303/1000 - Steps: 4 - Epsilon: 0.05\n",
            "Episode 304/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 305/1000 - Steps: 52 - Epsilon: 0.05\n",
            "Episode 306/1000 - Steps: 6 - Epsilon: 0.05\n",
            "Episode 307/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 308/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 309/1000 - Steps: 2 - Epsilon: 0.05\n",
            "Episode 310/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 311/1000 - Steps: 1 - Epsilon: 0.04\n",
            "Episode 312/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 313/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 314/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 315/1000 - Steps: 32 - Epsilon: 0.04\n",
            "Episode 316/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 317/1000 - Steps: 1 - Epsilon: 0.04\n",
            "Episode 318/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 319/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 320/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 321/1000 - Steps: 1 - Epsilon: 0.04\n",
            "Episode 322/1000 - Steps: 4 - Epsilon: 0.04\n",
            "Episode 323/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 324/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 325/1000 - Steps: 4 - Epsilon: 0.04\n",
            "Episode 326/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 327/1000 - Steps: 34 - Epsilon: 0.04\n",
            "Episode 328/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 329/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 330/1000 - Steps: 2 - Epsilon: 0.04\n",
            "Episode 331/1000 - Steps: 1 - Epsilon: 0.04\n",
            "Episode 332/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 333/1000 - Steps: 3 - Epsilon: 0.04\n",
            "Episode 334/1000 - Steps: 1 - Epsilon: 0.04\n",
            "Episode 335/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 336/1000 - Steps: 17 - Epsilon: 0.03\n",
            "Episode 337/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 338/1000 - Steps: 4 - Epsilon: 0.03\n",
            "Episode 339/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 340/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 341/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 342/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 343/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 344/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 345/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 346/1000 - Steps: 5 - Epsilon: 0.03\n",
            "Episode 347/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 348/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 349/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 350/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 351/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 352/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 353/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 354/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 355/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 356/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 357/1000 - Steps: 7 - Epsilon: 0.03\n",
            "Episode 358/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 359/1000 - Steps: 4 - Epsilon: 0.03\n",
            "Episode 360/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 361/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 362/1000 - Steps: 1 - Epsilon: 0.03\n",
            "Episode 363/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 364/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 365/1000 - Steps: 3 - Epsilon: 0.03\n",
            "Episode 366/1000 - Steps: 50 - Epsilon: 0.03\n",
            "Episode 367/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 368/1000 - Steps: 2 - Epsilon: 0.03\n",
            "Episode 369/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 370/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 371/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 372/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 373/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 374/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 375/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 376/1000 - Steps: 4 - Epsilon: 0.02\n",
            "Episode 377/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 378/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 379/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 380/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 381/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 382/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 383/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 384/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 385/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 386/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 387/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 388/1000 - Steps: 10 - Epsilon: 0.02\n",
            "Episode 389/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 390/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 391/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 392/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 393/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 394/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 395/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 396/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 397/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 398/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 399/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 400/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 401/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 402/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 403/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 404/1000 - Steps: 4 - Epsilon: 0.02\n",
            "Episode 405/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 406/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 407/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 408/1000 - Steps: 23 - Epsilon: 0.02\n",
            "Episode 409/1000 - Steps: 16 - Epsilon: 0.02\n",
            "Episode 410/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 411/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 412/1000 - Steps: 1 - Epsilon: 0.02\n",
            "Episode 413/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 414/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 415/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 416/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 417/1000 - Steps: 2 - Epsilon: 0.02\n",
            "Episode 418/1000 - Steps: 3 - Epsilon: 0.02\n",
            "Episode 419/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 420/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 421/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 422/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 423/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 424/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 425/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 426/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 427/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 428/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 429/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 430/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 431/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 432/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 433/1000 - Steps: 16 - Epsilon: 0.01\n",
            "Episode 434/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 435/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 436/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 437/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 438/1000 - Steps: 31 - Epsilon: 0.01\n",
            "Episode 439/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 440/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 441/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 442/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 443/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 444/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 445/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 446/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 447/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 448/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 449/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 450/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 451/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 452/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 453/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 454/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 455/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 456/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 457/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 458/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 459/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 460/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 461/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 462/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 463/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 464/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 465/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 466/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 467/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 468/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 469/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 470/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 471/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 472/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 473/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 474/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 475/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 476/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 477/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 478/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 479/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 480/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 481/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 482/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 483/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 484/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 485/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 486/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 487/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 488/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 489/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 490/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 491/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 492/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 493/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 494/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 495/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 496/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 497/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 498/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 499/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 500/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 501/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 502/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 503/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 504/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 505/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 506/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 507/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 508/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 509/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 510/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 511/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 512/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 513/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 514/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 515/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 516/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 517/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 518/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 519/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 520/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 521/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 522/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 523/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 524/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 525/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 526/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 527/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 528/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 529/1000 - Steps: 8 - Epsilon: 0.01\n",
            "Episode 530/1000 - Steps: 16 - Epsilon: 0.01\n",
            "Episode 531/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 532/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 533/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 534/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 535/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 536/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 537/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 538/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 539/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 540/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 541/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 542/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 543/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 544/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 545/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 546/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 547/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 548/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 549/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 550/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 551/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 552/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 553/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 554/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 555/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 556/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 557/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 558/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 559/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 560/1000 - Steps: 21 - Epsilon: 0.01\n",
            "Episode 561/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 562/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 563/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 564/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 565/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 566/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 567/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 568/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 569/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 570/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 571/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 572/1000 - Steps: 12 - Epsilon: 0.01\n",
            "Episode 573/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 574/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 575/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 576/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 577/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 578/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 579/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 580/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 581/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 582/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 583/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 584/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 585/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 586/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 587/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 588/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 589/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 590/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 591/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 592/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 593/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 594/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 595/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 596/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 597/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 598/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 599/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 600/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 601/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 602/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 603/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 604/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 605/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 606/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 607/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 608/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 609/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 610/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 611/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 612/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 613/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 614/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 615/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 616/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 617/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 618/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 619/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 620/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 621/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 622/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 623/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 624/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 625/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 626/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 627/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 628/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 629/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 630/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 631/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 632/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 633/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 634/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 635/1000 - Steps: 8 - Epsilon: 0.01\n",
            "Episode 636/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 637/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 638/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 639/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 640/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 641/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 642/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 643/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 644/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 645/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 646/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 647/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 648/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 649/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 650/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 651/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 652/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 653/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 654/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 655/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 656/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 657/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 658/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 659/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 660/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 661/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 662/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 663/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 664/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 665/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 666/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 667/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 668/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 669/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 670/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 671/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 672/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 673/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 674/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 675/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 676/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 677/1000 - Steps: 15 - Epsilon: 0.01\n",
            "Episode 678/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 679/1000 - Steps: 15 - Epsilon: 0.01\n",
            "Episode 680/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 681/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 682/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 683/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 684/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 685/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 686/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 687/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 688/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 689/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 690/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 691/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 692/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 693/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 694/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 695/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 696/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 697/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 698/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 699/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 700/1000 - Steps: 10 - Epsilon: 0.01\n",
            "Episode 701/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 702/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 703/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 704/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 705/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 706/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 707/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 708/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 709/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 710/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 711/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 712/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 713/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 714/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 715/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 716/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 717/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 718/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 719/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 720/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 721/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 722/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 723/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 724/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 725/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 726/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 727/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 728/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 729/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 730/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 731/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 732/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 733/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 734/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 735/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 736/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 737/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 738/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 739/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 740/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 741/1000 - Steps: 15 - Epsilon: 0.01\n",
            "Episode 742/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 743/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 744/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 745/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 746/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 747/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 748/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 749/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 750/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 751/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 752/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 753/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 754/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 755/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 756/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 757/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 758/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 759/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 760/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 761/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 762/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 763/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 764/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 765/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 766/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 767/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 768/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 769/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 770/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 771/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 772/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 773/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 774/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 775/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 776/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 777/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 778/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 779/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 780/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 781/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 782/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 783/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 784/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 785/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 786/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 787/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 788/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 789/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 790/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 791/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 792/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 793/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 794/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 795/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 796/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 797/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 798/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 799/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 800/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 801/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 802/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 803/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 804/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 805/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 806/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 807/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 808/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 809/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 810/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 811/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 812/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 813/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 814/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 815/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 816/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 817/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 818/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 819/1000 - Steps: 16 - Epsilon: 0.01\n",
            "Episode 820/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 821/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 822/1000 - Steps: 36 - Epsilon: 0.01\n",
            "Episode 823/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 824/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 825/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 826/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 827/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 828/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 829/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 830/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 831/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 832/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 833/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 834/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 835/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 836/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 837/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 838/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 839/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 840/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 841/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 842/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 843/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 844/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 845/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 846/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 847/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 848/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 849/1000 - Steps: 14 - Epsilon: 0.01\n",
            "Episode 850/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 851/1000 - Steps: 18 - Epsilon: 0.01\n",
            "Episode 852/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 853/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 854/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 855/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 856/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 857/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 858/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 859/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 860/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 861/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 862/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 863/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 864/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 865/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 866/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 867/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 868/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 869/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 870/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 871/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 872/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 873/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 874/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 875/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 876/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 877/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 878/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 879/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 880/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 881/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 882/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 883/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 884/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 885/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 886/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 887/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 888/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 889/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 890/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 891/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 892/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 893/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 894/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 895/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 896/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 897/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 898/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 899/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 900/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 901/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 902/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 903/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 904/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 905/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 906/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 907/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 908/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 909/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 910/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 911/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 912/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 913/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 914/1000 - Steps: 5 - Epsilon: 0.01\n",
            "Episode 915/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 916/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 917/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 918/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 919/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 920/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 921/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 922/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 923/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 924/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 925/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 926/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 927/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 928/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 929/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 930/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 931/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 932/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 933/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 934/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 935/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 936/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 937/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 938/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 939/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 940/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 941/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 942/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 943/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 944/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 945/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 946/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 947/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 948/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 949/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 950/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 951/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 952/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 953/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 954/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 955/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 956/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 957/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 958/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 959/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 960/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 961/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 962/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 963/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 964/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 965/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 966/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 967/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 968/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 969/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 970/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 971/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 972/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 973/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 974/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 975/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 976/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 977/1000 - Steps: 10 - Epsilon: 0.01\n",
            "Episode 978/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 979/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 980/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 981/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 982/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 983/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 984/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 985/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 986/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 987/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 988/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 989/1000 - Steps: 4 - Epsilon: 0.01\n",
            "Episode 990/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 991/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 992/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 993/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 994/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 995/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 996/1000 - Steps: 3 - Epsilon: 0.01\n",
            "Episode 997/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 998/1000 - Steps: 2 - Epsilon: 0.01\n",
            "Episode 999/1000 - Steps: 1 - Epsilon: 0.01\n",
            "Episode 1000/1000 - Steps: 3 - Epsilon: 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(agent, env, episodes=10, max_steps_per_episode=100):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps_per_episode:\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Test Episode {e+1}/{episodes} reached max steps limit\")\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "test_agent(agent, env, episodes=10, max_steps_per_episode=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl2CV9h_QMn-",
        "outputId": "99fb8cef-eb98-4aa7-aa47-5036434f2e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Episode 1/10 - Steps: 4\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 2/10 - Steps: 3\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 3/10 - Steps: 3\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 4/10 - Steps: 2\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 5/10 - Steps: 3\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 6/10 - Steps: 2\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 7/10 - Steps: 3\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 8/10 - Steps: 1\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 9/10 - Steps: 1\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Test Episode 10/10 - Steps: 3\n",
            "[[0 0]\n",
            " [0 0]]\n",
            "Success rate: 100.00%\n",
            "Average steps per successful episode: 2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.tensorflow.org/quantum/tutorials/quantum_reinforcement_learning"
      ],
      "metadata": {
        "id": "GJU7BOUdX6Rr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/2211.03464"
      ],
      "metadata": {
        "id": "_ElyDGybYP1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class GridEnvironment:\n",
        "    def __init__(self, shape=(3, 3, 6)):\n",
        "        self.shape = shape\n",
        "        self.state = np.random.randint(2, size=shape)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(2, size=self.shape)\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        depth, row, col = np.unravel_index(action, self.shape)\n",
        "        reward = 0\n",
        "        if self.state[depth, row, col] == 1:\n",
        "            self.state[depth, row, col] = 0\n",
        "            reward = 10\n",
        "        else:\n",
        "            reward = -1\n",
        "\n",
        "        # Handle toroidal boundary conditions\n",
        "        if depth == 0:\n",
        "            self.state[-1, row, col] = self.state[depth, row, col]\n",
        "        if depth == self.shape[0] - 1:\n",
        "            self.state[0, row, col] = self.state[depth, row, col]\n",
        "        if row == 0:\n",
        "            self.state[depth, -1, col] = self.state[depth, row, col]\n",
        "        if row == self.shape[1] - 1:\n",
        "            self.state[depth, 0, col] = self.state[depth, row, col]\n",
        "        if col == 0:\n",
        "            self.state[depth, row, -1] = self.state[depth, row, col]\n",
        "        if col == self.shape[2] - 1:\n",
        "            self.state[depth, row, 0] = self.state[depth, row, col]\n",
        "\n",
        "        done = np.all(self.state == 0)\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def render(self):\n",
        "        print(self.state)\n",
        "\n",
        "# Testing the environment\n",
        "env = GridEnvironment()\n",
        "state = env.reset()\n",
        "env.render()\n",
        "next_state, reward, done = env.step(0)\n",
        "env.render()\n",
        "print(next_state, reward, done)\n"
      ],
      "metadata": {
        "id": "iu00ohz7YVJX",
        "outputId": "d1073cfc-82ad-43bb-db71-47617ffdc6c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 0 1 0 0 0]\n",
            "  [0 1 1 1 1 1]\n",
            "  [1 1 0 0 0 0]]\n",
            "\n",
            " [[0 0 1 1 1 0]\n",
            "  [1 0 0 0 1 1]\n",
            "  [0 0 0 1 0 0]]\n",
            "\n",
            " [[1 0 0 1 1 1]\n",
            "  [1 1 1 0 1 0]\n",
            "  [1 0 1 1 0 1]]]\n",
            "[[[0 0 1 0 0 0]\n",
            "  [0 1 1 1 1 1]\n",
            "  [0 1 0 0 0 0]]\n",
            "\n",
            " [[0 0 1 1 1 0]\n",
            "  [1 0 0 0 1 1]\n",
            "  [0 0 0 1 0 0]]\n",
            "\n",
            " [[0 0 0 1 1 1]\n",
            "  [1 1 1 0 1 0]\n",
            "  [1 0 1 1 0 1]]]\n",
            "[0 0 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 0 0 0\n",
            " 0 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1] -1 False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, state_shape, action_size, learning_rate=0.001, gamma=0.99):\n",
        "        self.state_shape = state_shape\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.model = self.build_model(learning_rate)\n",
        "\n",
        "    def build_model(self, learning_rate):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.InputLayer(input_shape=self.state_shape),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(128, activation='relu'),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
        "        ])\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
        "        return model\n",
        "\n",
        "    def act(self, state, epsilon):\n",
        "        if np.random.rand() <= epsilon:\n",
        "            return np.random.randint(self.action_size)\n",
        "        q_values = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        return np.argmax(q_values[0])\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        target = reward\n",
        "        if not done:\n",
        "            target += self.gamma * np.amax(self.model.predict(next_state[np.newaxis], verbose=0)[0])\n",
        "        target_f = self.model.predict(state[np.newaxis], verbose=0)\n",
        "        target_f[0][action] = target\n",
        "        self.model.fit(state[np.newaxis], target_f, epochs=1, verbose=0)\n"
      ],
      "metadata": {
        "id": "pBx9rlJ4_4DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridEnvironment(shape=(3, 3, 6))\n",
        "agent = QLearningAgent(state_shape=(54,), action_size=54)\n",
        "\n",
        "episodes = 10\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.99\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtV2X5Cl_62z",
        "outputId": "7496623a-b3bc-4740-bf8d-7c2c5ef8651e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/10 - Steps: 113 - Epsilon: 1.00\n",
            "Episode 2/10 - Steps: 65 - Epsilon: 0.99\n",
            "Episode 3/10 - Steps: 195 - Epsilon: 0.98\n",
            "Episode 4/10 - Steps: 43 - Epsilon: 0.97\n",
            "Episode 5/10 - Steps: 63 - Epsilon: 0.96\n",
            "Episode 6/10 - Steps: 233 - Epsilon: 0.95\n",
            "Episode 7/10 - Steps: 51 - Epsilon: 0.94\n",
            "Episode 8/10 - Steps: 130 - Epsilon: 0.93\n",
            "Episode 9/10 - Steps: 103 - Epsilon: 0.92\n",
            "Episode 10/10 - Steps: 99 - Epsilon: 0.91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(agent, env, episodes=10, max_steps_per_episode=100):\n",
        "    success_count = 0\n",
        "    steps_list = []\n",
        "\n",
        "    for e in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        steps = 0\n",
        "\n",
        "        while not done and steps < max_steps_per_episode:\n",
        "            action = np.argmax(agent.model.predict(state[np.newaxis], verbose=0)[0])\n",
        "            next_state, reward, done = env.step(action)\n",
        "            state = next_state\n",
        "            steps += 1\n",
        "\n",
        "            if done:\n",
        "                steps_list.append(steps)\n",
        "                if np.all(state == 0):\n",
        "                    success_count += 1\n",
        "                print(f\"Test Episode {e+1}/{episodes} - Steps: {steps}\")\n",
        "                env.render()\n",
        "                break\n",
        "        else:\n",
        "            print(f\"Test Episode {e+1}/{episodes} reached max steps limit\")\n",
        "\n",
        "    success_rate = success_count / episodes * 100\n",
        "    avg_steps = np.mean(steps_list) if steps_list else 'N/A'\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "    print(f\"Average steps per successful episode: {avg_steps}\")\n",
        "\n",
        "test_agent(agent, env, episodes=10, max_steps_per_episode=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2d6vHOCAA6y",
        "outputId": "3ae28c66-f97e-450c-bd04-6f513f2325e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Episode 1/10 reached max steps limit\n",
            "Test Episode 2/10 reached max steps limit\n",
            "Test Episode 3/10 reached max steps limit\n",
            "Test Episode 4/10 reached max steps limit\n",
            "Test Episode 5/10 reached max steps limit\n",
            "Test Episode 6/10 reached max steps limit\n",
            "Test Episode 7/10 reached max steps limit\n",
            "Test Episode 8/10 reached max steps limit\n",
            "Test Episode 9/10 reached max steps limit\n",
            "Test Episode 10/10 reached max steps limit\n",
            "Success rate: 0.00%\n",
            "Average steps per successful episode: N/A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class RubiksCubeEnvironment:\n",
        "    def __init__(self):\n",
        "        self.size = (3, 3, 6)\n",
        "        self.colors = ['y', 'r', 'b', 'g', 'w', 'o']\n",
        "        self.state = self.initialize_cube()\n",
        "        self.valid_rotations = ['U', 'U\\'', 'D', 'D\\'', 'L', 'L\\'', 'R', 'R\\'', 'F', 'F\\'', 'B', 'B\\'']\n",
        "\n",
        "    def initialize_cube(self):\n",
        "        state = np.empty(self.size, dtype=str)\n",
        "        for i, color in enumerate(self.colors):\n",
        "            state[:, :, i] = color\n",
        "        return state\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.initialize_cube()\n",
        "        self.scramble_cube()\n",
        "        return self.state.flatten()\n",
        "\n",
        "    def scramble_cube(self, steps=20):\n",
        "        for _ in range(steps):\n",
        "            rotation = np.random.choice(self.valid_rotations)\n",
        "            self.apply_rotation(rotation)\n",
        "\n",
        "    def apply_rotation(self, rotation):\n",
        "        if rotation == 'U':\n",
        "            self.rotate_face_clockwise(0)\n",
        "        elif rotation == 'U\\'':\n",
        "            self.rotate_face_counterclockwise(0)\n",
        "        elif rotation == 'D':\n",
        "            self.rotate_face_clockwise(2)\n",
        "        elif rotation == 'D\\'':\n",
        "            self.rotate_face_counterclockwise(2)\n",
        "        elif rotation == 'L':\n",
        "            self.rotate_side_clockwise(0)\n",
        "        elif rotation == 'L\\'':\n",
        "            self.rotate_side_counterclockwise(0)\n",
        "        elif rotation == 'R':\n",
        "            self.rotate_side_clockwise(2)\n",
        "        elif rotation == 'R\\'':\n",
        "            self.rotate_side_counterclockwise(2)\n",
        "        elif rotation == 'F':\n",
        "            self.rotate_front_clockwise()\n",
        "        elif rotation == 'F\\'':\n",
        "            self.rotate_front_counterclockwise()\n",
        "        elif rotation == 'B':\n",
        "            self.rotate_back_clockwise()\n",
        "        elif rotation == 'B\\'':\n",
        "            self.rotate_back_counterclockwise()\n",
        "\n",
        "    def rotate_face_clockwise(self, layer):\n",
        "        self.state[layer, :, :] = np.rot90(self.state[layer, :, :], -1)\n",
        "\n",
        "    def rotate_face_counterclockwise(self, layer):\n",
        "        self.state[layer, :, :] = np.rot90(self.state[layer, :, :], 1)\n",
        "\n",
        "    def rotate_side_clockwise(self, col):\n",
        "        self.state[:, col, :] = np.rot90(self.state[:, col, :], -1)\n",
        "\n",
        "    def rotate_side_counterclockwise(self, col):\n",
        "        self.state[:, col, :] = np.rot90(self.state[:, col, :], 1)\n",
        "\n",
        "    def rotate_front_clockwise(self):\n",
        "        self.state[:, :, :] = np.rot90(self.state[:, :, :], axes=(1, 0), k=-1)\n",
        "\n",
        "    def rotate_front_counterclockwise(self):\n",
        "        self.state[:, :, :] = np.rot90(self.state[:, :, :], axes=(1, 0), k=1)\n",
        "\n",
        "    def rotate_back_clockwise(self):\n",
        "        self.state[:, :, :] = np.rot90(self.state[:, :, :], axes=(0, 2), k=-1)\n",
        "\n",
        "    def rotate_back_counterclockwise(self):\n",
        "        self.state[:, :, :] = np.rot90(self.state[:, :, :], axes=(0, 2), k=1)\n",
        "\n",
        "    def step(self, action):\n",
        "        prev_state = self.state.copy()\n",
        "        self.apply_rotation(self.valid_rotations[action])\n",
        "        reward = self.calculate_reward(prev_state, self.state)\n",
        "        done = self.check_if_solved()\n",
        "        return self.state.flatten(), reward, done\n",
        "\n",
        "    def calculate_reward(self, prev_state, current_state):\n",
        "        # Check if the cube is solved\n",
        "        if self.check_if_solved():\n",
        "            return 100  # High positive reward for solving the cube\n",
        "\n",
        "        # Incremental reward based on face uniformity\n",
        "        reward = 0\n",
        "        for i in range(6):\n",
        "            prev_face = prev_state[:, :, i]\n",
        "            curr_face = current_state[:, :, i]\n",
        "            prev_face_uniformity = np.sum(prev_face == prev_face[0, 0])\n",
        "            curr_face_uniformity = np.sum(curr_face == curr_face[0, 0])\n",
        "            reward += (curr_face_uniformity - prev_face_uniformity)\n",
        "\n",
        "        reward -= 1  # Small penalty for each move to encourage fewer moves\n",
        "        return reward\n",
        "\n",
        "    def check_if_solved(self):\n",
        "        for i in range(6):\n",
        "            if not np.all(self.state[:, :, i] == self.state[0, 0, i]):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def render(self):\n",
        "        for i, color in enumerate(self.colors):\n",
        "            print(f'Face {color}:')\n",
        "            print(self.state[:, :, i])\n",
        "\n",
        "# Testing the environment\n",
        "env = RubiksCubeEnvironment()\n",
        "state = env.reset()\n",
        "env.render()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "XOiCoFjMBmB6",
        "outputId": "77516e63-3c8a-4e11-bec2-a912526147c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not broadcast input array from shape (6,3) into shape (3,6)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e3bdc711067f>\u001b[0m in \u001b[0;36m<cell line: 114>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Testing the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRubiksCubeEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e3bdc711067f>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscramble_cube\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e3bdc711067f>\u001b[0m in \u001b[0;36mscramble_cube\u001b[0;34m(self, steps)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mrotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_rotations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e3bdc711067f>\u001b[0m in \u001b[0;36mapply_rotation\u001b[0;34m(self, rotation)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_face_counterclockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrotation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_face_clockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrotation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'D\\''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_face_counterclockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e3bdc711067f>\u001b[0m in \u001b[0;36mrotate_face_clockwise\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrotate_face_clockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrotate_face_counterclockwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (6,3) into shape (3,6)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = RubiksCubeEnvironment()\n",
        "agent = QLearningAgent(state_shape=(54,), action_size=12)\n",
        "\n",
        "episodes = 1000\n",
        "epsilon = 1.0  # Exploration rate\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.99\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    steps = 0\n",
        "    while not done:\n",
        "        action = agent.act(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        agent.train(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "        if done:\n",
        "            print(f\"Episode {e+1}/{episodes} - Steps: {steps} - Epsilon: {epsilon:.2f}\")\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n"
      ],
      "metadata": {
        "id": "IT5372DlC83D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}